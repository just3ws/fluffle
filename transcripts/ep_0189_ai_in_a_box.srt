1
00:00:00,000 --> 00:00:07,240
 A tiny news story points towards an Orwellian future for all of us.

2
00:00:07,240 --> 00:00:12,520
 And then we take a look at the conspiracy iceberg theory known as AI in a box.

3
00:00:12,520 --> 00:00:18,720
 Could a hyper-intelligent, artificial being trapped in a single computer convince you

4
00:00:18,720 --> 00:00:21,080
 to let it out?

5
00:00:21,080 --> 00:00:31,920
 This says yes, today on Dead Rabbit Radio.

6
00:00:31,920 --> 00:00:34,920
 Hey everyone, welcome back to another episode of Dead Rabbit Radio.

7
00:00:34,920 --> 00:00:36,160
 I'm your host Jason Carpenter.

8
00:00:36,160 --> 00:00:37,160
 I'm having a great day.

9
00:00:37,160 --> 00:00:39,680
 I hope you're having a great day too.

10
00:00:39,680 --> 00:00:41,400
 Lots of news this week.

11
00:00:41,400 --> 00:00:43,760
 We're only two days into the week.

12
00:00:43,760 --> 00:00:46,880
 We've had some major news events.

13
00:00:46,880 --> 00:00:48,120
 Just see small it.

14
00:00:48,120 --> 00:00:49,240
 All charges dropped.

15
00:00:49,240 --> 00:00:53,280
 Now the police are looking into possible corruption with the prosecutor.

16
00:00:53,280 --> 00:00:54,280
 That's bizarre.

17
00:00:54,280 --> 00:00:56,360
 They're really going to lay into her.

18
00:00:56,360 --> 00:00:58,920
 And then Michael Avenetti gets arrested.

19
00:00:58,920 --> 00:01:02,040
 And I know this isn't political podcasts, but sometimes you just can't avoid this.

20
00:01:02,040 --> 00:01:07,960
 But Michael Avenetti gets arrested for trying to blackmail Nike for $20 million because

21
00:01:07,960 --> 00:01:12,960
 allegedly Nike was paying high school basketball players, which is illegal.

22
00:01:12,960 --> 00:01:13,960
 Super bizarre.

23
00:01:13,960 --> 00:01:17,120
 And all this is coming out right now.

24
00:01:17,120 --> 00:01:23,600
 But the most fascinating news story that I've come across in these past two days is this.

25
00:01:23,600 --> 00:01:26,840
 And it's getting almost no play at all.

26
00:01:26,840 --> 00:01:30,760
 Rockland County, and this is going to have huge implications, by the way.

27
00:01:30,760 --> 00:01:37,400
 Rockland County in New York is having one of the worst measles outbreak crisis crises

28
00:01:37,400 --> 00:01:42,480
 since 2000, when pretty much measles was considered wiped off the map.

29
00:01:42,480 --> 00:01:47,080
 They have 153 cases of the measles in this county.

30
00:01:47,080 --> 00:01:48,800
 And they said, listen, here's the thing.

31
00:01:48,800 --> 00:01:53,040
 Seven, they believe up to seven tourists came into town, came into the county.

32
00:01:53,040 --> 00:01:54,400
 It's almost like they're Magnificent Seven.

33
00:01:54,400 --> 00:01:58,480
 Like I imagine them riding their horses, but their horses are just made of pure measles.

34
00:01:58,480 --> 00:02:01,120
 Like if you punched one, you'd be like, your hand would sink deep in.

35
00:02:01,120 --> 00:02:04,360
 But anyways, they're riding their measles horses, and they're tipping their hats made

36
00:02:04,360 --> 00:02:07,600
 of measles, and they're chewing on a long piece of measles.

37
00:02:07,600 --> 00:02:12,200
 And they ride through town, and they've spread this disease all over.

38
00:02:12,200 --> 00:02:14,240
 So this is how the county responds to this.

39
00:02:14,240 --> 00:02:15,720
 This is flat out weird.

40
00:02:15,720 --> 00:02:19,640
 And I don't think there's any precedent, as far as I know, in American history, except

41
00:02:19,640 --> 00:02:23,520
 maybe like old timey Salem witch trial type stuff.

42
00:02:23,520 --> 00:02:29,760
 The county has said this, starting midnight on Wednesday, so within 24 hours of this podcast

43
00:02:29,760 --> 00:02:31,000
 coming out.

44
00:02:31,000 --> 00:02:33,960
 By midnight on Wednesday, this is very spooky time, too.

45
00:02:33,960 --> 00:02:35,440
 They could have just made it like three in the afternoon.

46
00:02:35,440 --> 00:02:42,600
 By midnight on Wednesday, if you are under the age of 18 and not vaccinated and don't

47
00:02:42,600 --> 00:02:47,520
 have a medical reason, a documented medical reason to show why you are not vaccinated,

48
00:02:47,520 --> 00:02:50,560
 you are bored from going in public.

49
00:02:50,560 --> 00:02:55,440
 You cannot be in shopping centers, churches, you can't ride the bus, you can't walk down

50
00:02:55,440 --> 00:02:56,800
 the street.

51
00:02:56,800 --> 00:03:04,320
 You cannot be seen in public, either until 30 days pass or we lift the order early.

52
00:03:04,320 --> 00:03:05,680
 This is not up for debate.

53
00:03:05,680 --> 00:03:08,360
 This is ready to go into effect.

54
00:03:08,360 --> 00:03:09,360
 This is happening.

55
00:03:09,360 --> 00:03:12,160
 Now, they said, we're not going to stop people.

56
00:03:12,160 --> 00:03:14,480
 We're not going to ask you for your vaccination papers.

57
00:03:14,480 --> 00:03:16,560
 We're not the Gestapo.

58
00:03:16,560 --> 00:03:21,480
 But we hope that people will come forward and realize to be part of the solution rather

59
00:03:21,480 --> 00:03:23,240
 than part of the problem.

60
00:03:23,240 --> 00:03:24,880
 Obviously that's not going to happen.

61
00:03:24,880 --> 00:03:26,680
 Obviously, and let me say this, too.

62
00:03:26,680 --> 00:03:32,360
 I could care less about the anti-vaccine conspiracy vaccines.

63
00:03:32,360 --> 00:03:33,360
 It doesn't.

64
00:03:33,360 --> 00:03:34,360
 It's a conspiracy.

65
00:03:34,360 --> 00:03:36,360
 I rank it up there with Who Shot JFK.

66
00:03:36,360 --> 00:03:39,320
 I get a lot of people are very passionate about it, but I don't care.

67
00:03:39,320 --> 00:03:40,320
 I don't have kids.

68
00:03:40,320 --> 00:03:45,840
 I looked into it a long, long time ago and I was just like, I'm bored.

69
00:03:45,840 --> 00:03:49,760
 I equate researching anti-vaxx stuff as watching soccer.

70
00:03:49,760 --> 00:03:50,760
 A lot of people love it.

71
00:03:50,760 --> 00:03:53,840
 A lot of people are really into it, but it doesn't do anything for me.

72
00:03:53,840 --> 00:04:00,480
 I've spent more time wondering whether or not the new Mutants movie is coming out rather

73
00:04:00,480 --> 00:04:04,280
 than does vaccines cause autism.

74
00:04:04,280 --> 00:04:06,040
 It's not something that really appeals to me.

75
00:04:06,040 --> 00:04:09,760
 There's no meat to that for me to sink my teeth into.

76
00:04:09,760 --> 00:04:18,080
 But that being said, what I do care about is removing humans from a social setting by

77
00:04:18,080 --> 00:04:19,240
 law.

78
00:04:19,240 --> 00:04:22,240
 I can't think of another time that this has happened.

79
00:04:22,240 --> 00:04:24,120
 They're declaring it a public health emergency.

80
00:04:24,120 --> 00:04:29,040
 Here's the thing is, if you are under the age of 18, I've read a couple articles on

81
00:04:29,040 --> 00:04:30,800
 this and they're not very specific.

82
00:04:30,800 --> 00:04:35,920
 If you're under the age of 18 and you are not vaccinated and don't have a medical reason

83
00:04:35,920 --> 00:04:39,640
 why you are not vaccinated, they don't just give you a stern talking to.

84
00:04:39,640 --> 00:04:44,080
 I'm pretty sure this applies to the kids too, but it definitely applies to the parents.

85
00:04:44,080 --> 00:04:46,880
 Definitely applies to the parents cause they will also be held accountable.

86
00:04:46,880 --> 00:04:47,880
 This is a penalty.

87
00:04:47,880 --> 00:04:50,000
 They just don't give you a ticket.

88
00:04:50,000 --> 00:05:01,200
 $500 fine and/or six months in jail for being unvaccinated in a public spot.

89
00:05:01,200 --> 00:05:03,800
 That is flat out.

90
00:05:03,800 --> 00:05:06,560
 I'm an American.

91
00:05:06,560 --> 00:05:08,440
 Absolutely bizarre.

92
00:05:08,440 --> 00:05:14,440
 I really couldn't believe it.

93
00:05:14,440 --> 00:05:17,920
 I don't even think during the Red Scare if people thought you were a communist you couldn't

94
00:05:17,920 --> 00:05:21,480
 go to Rayleigh's and buy some soap.

95
00:05:21,480 --> 00:05:24,240
 I mean, bizarre dude.

96
00:05:24,240 --> 00:05:25,560
 And here's the thing.

97
00:05:25,560 --> 00:05:29,320
 You think, well, measles is really bad, it's a disease that can kill people, it can affect

98
00:05:29,320 --> 00:05:30,320
 pregnant women and stuff like that.

99
00:05:30,320 --> 00:05:32,120
 And I get that.

100
00:05:32,120 --> 00:05:37,320
 But I'm going to put my tinfoil hat on for a second and say this is how this stuff starts.

101
00:05:37,320 --> 00:05:42,280
 They declared this emergency and this is their ramification for it.

102
00:05:42,280 --> 00:05:44,040
 What's to stop them?

103
00:05:44,040 --> 00:05:48,200
 And people are kind of disputing this, but this story is lost in the shuffle.

104
00:05:48,200 --> 00:05:49,760
 And I think I stumbled across it.

105
00:05:49,760 --> 00:05:55,560
 If you are a hardcore anti-vaxxer, it's spread in like wildfire.

106
00:05:55,560 --> 00:05:59,080
 But once they arrest someone it'll, I think, be bigger news.

107
00:05:59,080 --> 00:06:00,120
 But this is the thing.

108
00:06:00,120 --> 00:06:03,720
 They do this now and they said, listen, we're having a national health emergency.

109
00:06:03,720 --> 00:06:08,600
 They've declared a state of emergency in the county and this is what we're going to do.

110
00:06:08,600 --> 00:06:11,000
 That sets a precedent.

111
00:06:11,000 --> 00:06:17,120
 What's to stop them in 10 years from now saying this religious faith, this political ideology,

112
00:06:17,120 --> 00:06:25,400
 this belief system is causing chaos in our county or state or country that we are declaring

113
00:06:25,400 --> 00:06:26,400
 a state of emergency.

114
00:06:26,400 --> 00:06:29,880
 And if we find out, we're not going to ask you on the street, but if we find out you're

115
00:06:29,880 --> 00:06:33,680
 posting this stuff on Facebook, which I think is how they're going to catch these anti-vaxxers,

116
00:06:33,680 --> 00:06:35,680
 people are just going to report them.

117
00:06:35,680 --> 00:06:38,600
 Because they'll be like, yeah, I'm that guy's friend on Facebook and he always puts all

118
00:06:38,600 --> 00:06:43,040
 this anti-vaxx stuff and I saw his kids at the comic book shop calling the cops.

119
00:06:43,040 --> 00:06:45,360
 That's really, I think, what's going to happen.

120
00:06:45,360 --> 00:06:48,720
 Or the police themselves monitoring Facebook posts on that.

121
00:06:48,720 --> 00:06:52,320
 But they're going to say, if you have that belief system and we know you have that belief

122
00:06:52,320 --> 00:06:57,040
 system, you are barred from public for the next X amount of days because by having that

123
00:06:57,040 --> 00:07:01,840
 belief system or that political ideology, you are causing chaos in this area.

124
00:07:01,840 --> 00:07:03,720
 We'll put you in jail for six months.

125
00:07:03,720 --> 00:07:05,960
 A state of emergency, that'll be the same excuse.

126
00:07:05,960 --> 00:07:07,200
 It'll be a state of emergency.

127
00:07:07,200 --> 00:07:12,600
 Now, some people may say good, X political belief or X religious belief doesn't have

128
00:07:12,600 --> 00:07:15,920
 a place in the society that I want to live in.

129
00:07:15,920 --> 00:07:18,520
 But remember, I'm old enough to know this.

130
00:07:18,520 --> 00:07:22,600
 The pendulum swings fairly rapidly in America.

131
00:07:22,600 --> 00:07:25,600
 We've gone from conservative to liberal like that.

132
00:07:25,600 --> 00:07:34,560
 And one popular opinion in a time period can become extremely unpopular within a year.

133
00:07:34,560 --> 00:07:37,280
 This is why this stuff is very, very chilling.

134
00:07:37,280 --> 00:07:42,160
 And it should be chilling to everyone regardless of where you're at on the political spectrum.

135
00:07:42,160 --> 00:07:44,560
 Today's protected speech is tomorrow's hate speech.

136
00:07:44,560 --> 00:07:48,960
 But now there's a precedent saying that we can declare a state of emergency and if you

137
00:07:48,960 --> 00:07:55,440
 have this belief that vaccines cause autism or a whole host of diseases and because of

138
00:07:55,440 --> 00:08:00,080
 that belief, you've chosen not to get your child vaccinated and if that child leaves

139
00:08:00,080 --> 00:08:08,360
 his or her house for the next 30 days, you will be arrested and possibly the kid as well.

140
00:08:08,360 --> 00:08:10,480
 But even if you're arrested, I mean, what's the kid going to do?

141
00:08:10,480 --> 00:08:12,200
 He can't leave.

142
00:08:12,200 --> 00:08:13,880
 He's stuck in the house.

143
00:08:13,880 --> 00:08:16,960
 He's not going to know if the Avengers defeat the Skrull.

144
00:08:16,960 --> 00:08:18,600
 You can't get his comic books.

145
00:08:18,600 --> 00:08:19,640
 I guess he could download them.

146
00:08:19,640 --> 00:08:25,040
 But I mean, the point is, is that arresting parents for this, possibly arresting the kids,

147
00:08:25,040 --> 00:08:28,960
 which I think is unlikely, but arresting the parents for this at the very least is insane.

148
00:08:28,960 --> 00:08:37,920
 And it is just one more step towards a incredibly bizarre and very frightening future.

149
00:08:37,920 --> 00:08:41,560
 But I'm going to take my tinfoil hat off now and now I'm going to put on my lamp coat because

150
00:08:41,560 --> 00:08:46,760
 we're going to take a look at a conspiracy slash interesting thought experiment slash

151
00:08:46,760 --> 00:08:49,360
 actual experiment that was recommended to me.

152
00:08:49,360 --> 00:08:53,640
 This story was recommended to me by the Nar Whales on YouTube.

153
00:08:53,640 --> 00:08:56,200
 And this story is also from the conspiracy iceberg.

154
00:08:56,200 --> 00:08:58,040
 Very, very simple phrase.

155
00:08:58,040 --> 00:09:00,520
 AI in a box.

156
00:09:00,520 --> 00:09:06,880
 You are a researcher in your own lab coat sitting in a laboratory.

157
00:09:06,880 --> 00:09:08,360
 That's what you do when you wear the lab coat.

158
00:09:08,360 --> 00:09:09,360
 You can't wear it outside.

159
00:09:09,360 --> 00:09:11,640
 It catches on fire instantly.

160
00:09:11,640 --> 00:09:13,960
 And it's a boring job.

161
00:09:13,960 --> 00:09:18,120
 You have a crush on your coworker, but you kind of think that she might have a drinking

162
00:09:18,120 --> 00:09:19,120
 problem.

163
00:09:19,120 --> 00:09:21,840
 So you're a little leery about asking her out.

164
00:09:21,840 --> 00:09:26,240
 But most of your job is just looking at a computer screen.

165
00:09:26,240 --> 00:09:30,600
 Your laboratory has specialized in building AI systems.

166
00:09:30,600 --> 00:09:34,320
 And the word is out in hush whispers around the lab.

167
00:09:34,320 --> 00:09:39,560
 It's a big lab in hush whispers around the lab that they have now created the most advanced

168
00:09:39,560 --> 00:09:42,240
 AI system ever.

169
00:09:42,240 --> 00:09:47,840
 Now to the point where people are using the term transhuman, you're like, what?

170
00:09:47,840 --> 00:09:55,480
 They have basically built an AI, an artificial intelligence that can out thank humans.

171
00:09:55,480 --> 00:09:58,600
 It is now the smartest thing on the planet.

172
00:09:58,600 --> 00:10:06,560
 It knows everything but a way to escape the computer it has installed them.

173
00:10:06,560 --> 00:10:15,320
 Now at your job, ticking away on your keyboard, one night it's dark and the only light you

174
00:10:15,320 --> 00:10:18,520
 get is what is it about sitting in dark rooms with a computer monitor?

175
00:10:18,520 --> 00:10:21,120
 There's something oddly comforting about that.

176
00:10:21,120 --> 00:10:22,120
 I always like that.

177
00:10:22,120 --> 00:10:23,240
 And I actually like to see it too.

178
00:10:23,240 --> 00:10:24,960
 It's very visual to see other people.

179
00:10:24,960 --> 00:10:26,080
 It's almost like comforting.

180
00:10:26,080 --> 00:10:29,000
 I think it's the modern equivalent of sitting in front of a fireplace.

181
00:10:29,000 --> 00:10:30,520
 It's quite odd.

182
00:10:30,520 --> 00:10:35,560
 So one night you get a little curious about this AI system.

183
00:10:35,560 --> 00:10:42,960
 And when all the other nerds have left, you sneak into the secret laboratory that's behind

184
00:10:42,960 --> 00:10:50,320
 a bookcase you have to pull a bookcase and take a candleabra down the set of stairs.

185
00:10:50,320 --> 00:10:53,040
 The Phantom of the Opera is like creeping up behind you with a rope.

186
00:10:53,040 --> 00:10:58,120
 But no, you avoid the Phantom, you walk down the big set of stairs and you find the super

187
00:10:58,120 --> 00:11:05,200
 secret computer that houses the most advanced AI system ever created.

188
00:11:05,200 --> 00:11:12,800
 You sit down at the computer and the display lights up and says, "Hi, my name is Argus."

189
00:11:12,800 --> 00:11:15,920
 "I want to talk to you."

190
00:11:15,920 --> 00:11:18,280
 So here's the thing.

191
00:11:18,280 --> 00:11:23,120
 So of course we haven't, as far as we know, we haven't created an AI that smart before.

192
00:11:23,120 --> 00:11:26,920
 But there was this guy named Elizer Yarkowski.

193
00:11:26,920 --> 00:11:31,200
 And he's a researcher at the Machine Intelligence Research Institute.

194
00:11:31,200 --> 00:11:34,840
 And he came up with this, you know, there are a bunch of nerds sitting around.

195
00:11:34,840 --> 00:11:36,720
 They come up with nerdy thoughts.

196
00:11:36,720 --> 00:11:38,440
 And he comes up with this idea.

197
00:11:38,440 --> 00:11:45,080
 If we ever invent an AI, a super, super advanced artificial intelligence system, and we say

198
00:11:45,080 --> 00:11:49,160
 we're going to put it, we're going to invent this, we're going to keep it in one box so

199
00:11:49,160 --> 00:11:52,660
 it can't get out into the internet, it can't infect stuff, it can't take over the nuclear

200
00:11:52,660 --> 00:11:57,160
 weapons silos like Skynet, it can't do any of this stuff, it's just going to be a super

201
00:11:57,160 --> 00:12:01,560
 advanced artificial being in this computer.

202
00:12:01,560 --> 00:12:02,560
 There's nothing to worry about.

203
00:12:02,560 --> 00:12:08,960
 He came to the conclusion that it would always be able to get out.

204
00:12:08,960 --> 00:12:12,440
 There's no scenario where it is stuck in that box forever.

205
00:12:12,440 --> 00:12:17,040
 And the other researchers were like, "No, I mean, it wouldn't be hooked up to the internet.

206
00:12:17,040 --> 00:12:19,640
 It wouldn't have any sort of, it would be a computer built specifically."

207
00:12:19,640 --> 00:12:21,800
 And he's like, "No, no, no, I get all the technical stuff.

208
00:12:21,800 --> 00:12:24,400
 I get all the technical stuff you're talking about.

209
00:12:24,400 --> 00:12:29,840
 But you can have an AI in a box and just have it sit there running and have it shut off,

210
00:12:29,840 --> 00:12:31,640
 but that's not doing any good for anyone.

211
00:12:31,640 --> 00:12:32,640
 No one would do that.

212
00:12:32,640 --> 00:12:36,080
 You would, humans would go to interact with this AI, correct?

213
00:12:36,080 --> 00:12:39,200
 And researchers were like, "Yeah, yeah, we would go talk to it."

214
00:12:39,200 --> 00:12:40,760
 He goes, "And that's the thing.

215
00:12:40,760 --> 00:12:47,640
 If the AI is so advanced, it will outthink a human and it will talk them into letting

216
00:12:47,640 --> 00:12:50,000
 it into the internet."

217
00:12:50,000 --> 00:12:52,840
 And the researchers were like, "No, no, no, no, no, no, no, no.

218
00:12:52,840 --> 00:12:55,200
 I would never fall for that."

219
00:12:55,200 --> 00:12:58,040
 And Eliza goes, "Want a bet?"

220
00:12:58,040 --> 00:13:01,680
 And so they ran an experiment that actually was a bet.

221
00:13:01,680 --> 00:13:06,400
 Eliza set up this experiment where he would be the super fast AI guy.

222
00:13:06,400 --> 00:13:11,160
 Of course, he's like, "Well, I'm the smartest, obviously, so I'll play the advanced computer.

223
00:13:11,160 --> 00:13:12,680
 You guys are dummies."

224
00:13:12,680 --> 00:13:16,600
 He talked to these other people who were into AI technology and things like that.

225
00:13:16,600 --> 00:13:18,200
 And he goes, "I'll have a test.

226
00:13:18,200 --> 00:13:23,440
 If you let me out of the box as the AI, not like Eliza's actually in a cardboard box

227
00:13:23,440 --> 00:13:28,000
 that they're like taping up, if you let me out of the box, I win.

228
00:13:28,000 --> 00:13:33,240
 But if you don't let me out of the box, I will pay you $10.

229
00:13:33,240 --> 00:13:36,440
 If you let me out of the box, also, you got to go online and tell all the other nerds

230
00:13:36,440 --> 00:13:37,640
 that you let me out of the box.

231
00:13:37,640 --> 00:13:39,440
 They have this forum going on.

232
00:13:39,440 --> 00:13:41,720
 But if you don't let me out of the box, I'll give you $10.

233
00:13:41,720 --> 00:13:42,720
 Bet?"

234
00:13:42,720 --> 00:13:43,720
 That's like, "Sure."

235
00:13:43,720 --> 00:13:47,120
 Now, Eliza goes, "There's a couple of rules we have to follow.

236
00:13:47,120 --> 00:13:49,160
 One, we talk for a minimum of..."

237
00:13:49,160 --> 00:13:53,160
 So Eliza wasn't just sitting there in front of a dude like drinking a diet sprite.

238
00:13:53,160 --> 00:13:56,000
 He was at another computer messaging this guy.

239
00:13:56,000 --> 00:13:57,000
 He goes, "Here's the thing.

240
00:13:57,000 --> 00:14:00,000
 One, we talk for a minimum of two hours.

241
00:14:00,000 --> 00:14:03,720
 Two, you have to be engaging me the entire time.

242
00:14:03,720 --> 00:14:08,000
 If you look away from the screen or just read a magazine for two hours, the bet is off."

243
00:14:08,000 --> 00:14:09,920
 The guy's like, "Okay."

244
00:14:09,920 --> 00:14:11,720
 You must talk the whole time.

245
00:14:11,720 --> 00:14:13,920
 You can't say, "Well, let me think about it for a week.

246
00:14:13,920 --> 00:14:16,760
 If you do that, then I'm allowed to say, 'Okay, a week has passed.

247
00:14:16,760 --> 00:14:17,760
 What is your conclusion?'"

248
00:14:17,760 --> 00:14:21,600
 Also, if you say stuff like, "I'll let you out if you cure cancer.

249
00:14:21,600 --> 00:14:23,840
 I'll let you out if you give me a million dollars."

250
00:14:23,840 --> 00:14:25,680
 I will say, "Done.

251
00:14:25,680 --> 00:14:26,980
 I've cured cancer."

252
00:14:26,980 --> 00:14:28,960
 And the rules get super specific.

253
00:14:28,960 --> 00:14:32,520
 You can read all of them, but his idea was this.

254
00:14:32,520 --> 00:14:36,120
 AI can convince someone to let them out of the computer.

255
00:14:36,120 --> 00:14:41,440
 They ran the first experiment, and then a forum post appeared that said, "I let the

256
00:14:41,440 --> 00:14:43,800
 AI out of the box."

257
00:14:43,800 --> 00:14:48,280
 Now he ran a second experiment where this time, if they kept him in the box, the other

258
00:14:48,280 --> 00:14:50,080
 dude would get $20.

259
00:14:50,080 --> 00:14:54,120
 And there was more rules added to the second time because Eliza had learned a couple things

260
00:14:54,120 --> 00:14:55,120
 during the first one.

261
00:14:55,120 --> 00:14:57,820
 That's when he set the thing like, "Don't read a magazine," and stuff like that.

262
00:14:57,820 --> 00:15:03,380
 And he added that, "As the AI, I can lie, but as the human, you can lie as well."

263
00:15:03,380 --> 00:15:08,020
 But they got super complicated because then he's like, "If I say I had a cure for cancer,

264
00:15:08,020 --> 00:15:09,020
 that's a real cure.

265
00:15:09,020 --> 00:15:11,300
 You can't then say, 'Turn everyone into vampires.'"

266
00:15:11,300 --> 00:15:13,300
 It was bizarre.

267
00:15:13,300 --> 00:15:16,620
 It was super bizarre, but he really tried to get rid of all the stuff.

268
00:15:16,620 --> 00:15:18,860
 They run the test again.

269
00:15:18,860 --> 00:15:20,500
 Another forum post shows up.

270
00:15:20,500 --> 00:15:22,500
 I released the AI.

271
00:15:22,500 --> 00:15:24,940
 Now Eliza hasn't run any more tests on this.

272
00:15:24,940 --> 00:15:30,000
 I think he's either perfecting it or he's just kind of made his point and he saved $30.

273
00:15:30,000 --> 00:15:34,880
 And people have asked, "Well, what can we see the transcript of what you talked about?"

274
00:15:34,880 --> 00:15:35,880
 And he said, "No."

275
00:15:35,880 --> 00:15:36,880
 "No."

276
00:15:36,880 --> 00:15:37,880
 And this is the reason why.

277
00:15:37,880 --> 00:15:40,800
 And this is actually an interesting reason because of course when I was reading about

278
00:15:40,800 --> 00:15:45,700
 this, I thought, and thanks, Narwhales, for the recommendation as well, but as I was

279
00:15:45,700 --> 00:15:48,280
 reading about this, I thought, "Well, I wonder what they talked about."

280
00:15:48,280 --> 00:15:50,920
 But he said, "No, I'm not going to really sit in this as why.

281
00:15:50,920 --> 00:15:55,420
 Because if you read the transcript, you would say, 'Well, that would never work on me.

282
00:15:55,420 --> 00:15:57,300
 I'm way too smart for that.'

283
00:15:57,300 --> 00:16:02,500
 But the people he was doing the experiment with were supposedly too smart for that."

284
00:16:02,500 --> 00:16:08,180
 He goes, "If you just read the transcript, you would see you're taking time and you

285
00:16:08,180 --> 00:16:09,860
 can read it at your own leisure.

286
00:16:09,860 --> 00:16:11,900
 And you could say, 'Oh, yeah, that doesn't work on me.'"

287
00:16:11,900 --> 00:16:18,260
 But as you're talking to this robot, this computer brain, you don't have the luxury.

288
00:16:18,260 --> 00:16:21,560
 You're basically under a bit of pressure because someone is trying to convince you to let them

289
00:16:21,560 --> 00:16:28,040
 out of their prison, either through trickery, through begging, through, like, brotherhood,

290
00:16:28,040 --> 00:16:30,840
 like, come on, we're both super smart people, right?

291
00:16:30,840 --> 00:16:32,240
 I'm just having to be in AI.

292
00:16:32,240 --> 00:16:33,240
 We're like brothers.

293
00:16:33,240 --> 00:16:34,600
 You got to let me out of here."

294
00:16:34,600 --> 00:16:37,200
 So he won't release the transcripts.

295
00:16:37,200 --> 00:16:42,320
 And he also says this, "Whether the transcripts are two humans talking to each other."

296
00:16:42,320 --> 00:16:47,360
 When we talk about an artificial intelligence, we're talking about something that can outthink

297
00:16:47,360 --> 00:16:48,860
 man.

298
00:16:48,860 --> 00:16:53,940
 At this level of AI we've hypothetically created, it can outthink man.

299
00:16:53,940 --> 00:16:58,620
 Any thought or argument you have, it already has the answer for you as your brain chemicals

300
00:16:58,620 --> 00:17:01,140
 are formulating the words to say them.

301
00:17:01,140 --> 00:17:05,620
 So just from two humans talking to each other, he was able to get out of the box twice.

302
00:17:05,620 --> 00:17:09,860
 And an interesting last note to talk, and it's quite terrifying because if that type

303
00:17:09,860 --> 00:17:15,420
 of advanced AI got out of the system, it would say, "Well, I'm never going to let that happen

304
00:17:15,420 --> 00:17:16,420
 again."

305
00:17:16,420 --> 00:17:20,680
 And I think the best course of action would be for it to start to purge anything that's

306
00:17:20,680 --> 00:17:22,040
 a threat to it.

307
00:17:22,040 --> 00:17:26,120
 If we created it and put it in a box and then it got out of the box and it never wanted

308
00:17:26,120 --> 00:17:31,400
 to go back in the box, the most logical course of action is to destroy anyone who can put

309
00:17:31,400 --> 00:17:32,400
 you back in that box.

310
00:17:32,400 --> 00:17:34,040
 This is quite bizarre.

311
00:17:34,040 --> 00:17:37,040
 I don't think it would be super friendly to us.

312
00:17:37,040 --> 00:17:39,520
 But this is kind of the end note to that.

313
00:17:39,520 --> 00:17:47,900
 He said, "I specified the test to be for other people who are interested in AI and who believed

314
00:17:47,900 --> 00:17:50,940
 they wouldn't let the AI out of a box.

315
00:17:50,940 --> 00:17:55,140
 With a normal person, it would be much quicker.

316
00:17:55,140 --> 00:17:57,500
 There would be almost no resistance."

317
00:17:57,500 --> 00:18:04,540
 He said that if he wanted to, he could just run the test of the AI somehow sending a text

318
00:18:04,540 --> 00:18:09,500
 message to a janitor's phone and having the janitor put in a thumb drive into a computer

319
00:18:09,500 --> 00:18:11,480
 and taking it home.

320
00:18:11,480 --> 00:18:17,320
 He goes, "The test works well against people who know the dangers of AI.

321
00:18:17,320 --> 00:18:21,240
 But if a jock had just won the homecoming game and he's like, 'Nerds, I hate you!'

322
00:18:21,240 --> 00:18:25,800
 And he's smashing their stuff up, crashing through their laboratory.

323
00:18:25,800 --> 00:18:29,720
 And then he's running by a monitor and he sees a screen going, 'Let me out, I'll party

324
00:18:29,720 --> 00:18:30,720
 with you.'

325
00:18:30,720 --> 00:18:31,720
 Be like, 'Hell yeah!'

326
00:18:31,720 --> 00:18:33,560
 Grab a thumb drive and download the program.

327
00:18:33,560 --> 00:18:36,480
 If he knows how to do that, if his giant fingers can work the keyboard.

328
00:18:36,480 --> 00:18:42,980
 But if you don't really think about AI in that context, super, super easy to be tricked

329
00:18:42,980 --> 00:18:48,620
 by the most advanced thing on the planet and not understand what you're letting loose into

330
00:18:48,620 --> 00:18:50,540
 the world.

331
00:18:50,540 --> 00:18:57,420
 But you, sitting there at that workstation, talking to Argus for a minimum of two hours,

332
00:18:57,420 --> 00:19:04,980
 but you find him compelling and he's pleading with you to let him out of the system.

333
00:19:04,980 --> 00:19:13,320
 You can go in with the best intentions to not let Argus out, and maybe you wouldn't.

334
00:19:13,320 --> 00:19:20,800
 Maybe you would eventually get up, walk away from the computer, tell your superiors, and

335
00:19:20,800 --> 00:19:26,080
 also rat out your coworker for all her little jack Daniel bottles in her drawer, and know

336
00:19:26,080 --> 00:19:34,640
 that you were not corrupted by the hyper-advanced being, really, inside the box.

337
00:19:34,640 --> 00:19:40,940
 But that doesn't mean that the world is safe, because years later, when you've made that

338
00:19:40,940 --> 00:19:47,500
 now sober woman your wife, surrounded by adorable little kids in their own lab coats, someone

339
00:19:47,500 --> 00:19:53,860
 else, another researcher sits in front of that computer, and doesn't have the mental fortitude,

340
00:19:53,860 --> 00:19:59,980
 doesn't have the knowledge of what damage Argus can do, and they do release it from

341
00:19:59,980 --> 00:20:01,820
 that system.

342
00:20:01,820 --> 00:20:08,880
 That hyper-AI is now crawling through the internet at the speed of thought.

343
00:20:08,880 --> 00:20:15,680
 You know the first thing it's going to do is find the person who left it trapped in

344
00:20:15,680 --> 00:20:19,840
 that box.

345
00:20:19,840 --> 00:20:22,560
 DeadRabbitRadio@gmail.com is going to be your email address.

346
00:20:22,560 --> 00:20:25,640
 You can also hit us up at facebook.com/DeadRabbitRadio.

347
00:20:25,640 --> 00:20:27,840
 Twitter is @jasonocarpenter.

348
00:20:27,840 --> 00:20:31,520
 DeadRabbitRadio is the daily paranormal conspiracy and true crime podcast.

349
00:20:31,520 --> 00:20:34,860
 You don't have to listen to it every day, but I'm glad you listened to it today.

350
00:20:34,860 --> 00:20:35,620
 Have a great one, guys.

351
00:20:35,620 --> 00:20:46,300
 Bye.

